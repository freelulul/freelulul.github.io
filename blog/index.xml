<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Blogs on heyo</title>
    <link>https://freelulul.github.io/blog/</link>
    <description>Recent content in Blogs on heyo</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <managingEditor>zongzel@uchicago.edu (Zongze Li)</managingEditor>
    <webMaster>zongzel@uchicago.edu (Zongze Li)</webMaster>
    <lastBuildDate>Sat, 08 Jun 2024 01:48:36 +0800</lastBuildDate>
    <atom:link href="https://freelulul.github.io/blog/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Hetero_Comp_AIsys_intro</title>
      <link>https://freelulul.github.io/blog/Hetero_Comp_AIsys_intro/</link>
      <pubDate>Sat, 08 Jun 2024 01:48:36 +0800</pubDate><author>zongzel@uchicago.edu (Zongze Li)</author>
      <guid>https://freelulul.github.io/blog/Hetero_Comp_AIsys_intro/</guid>
      <description>超异构计算(DONE) 在这一节中我们要从更远的视角来看看计算机架构发展的黄金 10 年，主要将围绕异构计算和超异构来展开。在开始具体内容前，我们非常推荐您观看以下两个视频：&#xA;计算机架构的新黄金时代：A New Golden Age for Computer Architecture&#xA;编译器的黄金时代：The Golden Age of Compiler Design in an Era of HW/SW Co-design&#xA;他们可以让您更细致的了解计算机体系结构和编译器的发展历程和重要节点。接下来让我们先简述一下关于 AI 芯片发展的阶段。&#xA;AI 芯片发展 人工智能的发展与芯片算力的提升密不可分，可以大致分为三个阶段：&#xA;第一阶段：芯片算力不足，神经网络没有被受到重视 在早期，受限于芯片算力，复杂的神经网络模型难以实现。这一时期的人工智能主要依赖于专家系统、决策树等传统方法。神经网络虽然在理论上已经被提出，但由于计算资源的匮乏，难以训练出有效的模型，因此没有受到广泛重视。&#xA;第二阶段：CPU 算力大幅提升，但仍然无法满足神经网络增长需求 随着摩尔定律的推进，CPU 性能不断提升。这为神经网络的发展提供了一定的计算基础。研究者们开始尝试更大规模的神经网络，并在一些领域取得了突破。但是，神经网络对算力的需求呈指数级增长，单纯依靠 CPU 的性能提升已经难以满足日益复杂的模型训练需求。&#xA;第三阶段：GPU 和 AI 芯片新架构推动人工智能快速落地 为了解决算力瓶颈，研究者们开始将目光转向了其他计算架构。GPU 凭借其强大的并行计算能力，成为了深度学习的主要计算平台。与 CPU 相比，GPU 在矩阵运算等方面有着显著的优势，能够大幅加速神经网络训练。与此同时，一些专门针对 AI 加速的芯片架构也开始涌现，如 TPU、NPU 等。这些芯片在算力、功耗等方面进一步优化，为人工智能的落地应用扫清了障碍。&#xA;除了芯片算力外，算法的进步、数据的积累也是人工智能发展的重要推动力。在算力瓶颈得到缓解后，一些重要的神经网络结构如 CNN、RNN、Transformer 等被相继提出，并在图像、语音、自然语言处理等领域取得了突破性进展。海量的数据为模型训练提供了丰富的素材，使得神经网络能够学习到更加鲁棒和泛化的特征表示。&#xA;而更进一步，单一架构的使用也渐渐满足不了一些应用场景，针对于此，异构计算的概念也就应运而生。&#xA;异构与超异构场景 首先让我们来理解一下为什么需要异构？摩尔定律放缓，传统单一架构难以满足日益增长的计算需求。异构计算，犹如打破计算藩篱的利器，通过整合不同类型计算单元的优势，为计算难题提供全新的解决方案。&#xA;异构计算的主要优势有：&#xA;性能飞跃： 异构架构将 CPU、GPU、FPGA 等计算单元有机结合，充分发挥各自优势，实现 1+1&amp;gt;2 的效果，显著提升计算性能。&#xA;灵活定制： 针对不同计算任务，灵活选择合适的主张计算单元，实现资源的高效利用。&#xA;降低成本： 相比于昂贵的专用计算单元，异构架构用更低的成本实现更高的性能，带来更佳的性价比。&#xA;降低功耗： 异构架构能够根据任务需求动态调整资源分配，降低整体功耗，提升能源利用效率。&#xA;其应用场景也十分广泛，包括人工智能、高性能计算、大数据分析、图形处理等等&#xA;我们以一个具体的例子来引入：特斯拉 HW3 FSD 芯片（如下图），我们可以看到其单一芯片却有着 CPU，GPU，NPU 多种架构。&#xA;其之所以有如此异构架构则是由需求（如下图所标示）决定的：身为汽车芯片，其要负责雷达、GPS、地图等等多种功能，这时单一传统的架构就会比较难以高效完成任务：&#xA;而将各部分组件有机结合的异构芯片就可以更好的处理复杂情况，如下图，我们可以看到 GPU、NPU、Quad Cluster 等等硬件均被集合在一起，通过芯片外的 CPU 等等进行协同控制，这样就可以在多种任务的处理和切换时实现非常好的效果。&#xA;计算体系迎来异构 异构计算的出现和发展源于传统冯·诺依曼结构计算机受制于存储和计算单元之间的数据交换瓶颈，难以满足日益增长的计算需求，加之半导体工艺的发展使得 CPU 主频提升受到物理和功耗的限制，性能提升趋于缓慢。&#xA;为了突破单核 CPU 性能的瓶颈，业界开始探索并行计算技术，通过多核处理器或集群计算机实现高性能计算，然而并行计算中的微处理器仍受冯·诺依曼结构的制约，在处理数据密集型任务时，计算速度和性价比不尽如人意。&#xA;随着深度学习等人工智能技术的兴起，对计算能力提出了更高的要求，传统的 CPU 在处理神经网络训练和推理任务时，性能和效率远不及专门设计的 AI 芯片，如 GPU 和 NPU 等。异构计算通过集成不同类型的计算单元，发挥各自的计算优势，实现更高的性能和能效，AI 芯片在处理特定任务时，计算效率远超传统 CPU，有望成为未来计算机体系的标配。</description>
    </item>
    <item>
      <title>NPUBase_AIsys_intro</title>
      <link>https://freelulul.github.io/blog/NPUBase_AIsys_intro/</link>
      <pubDate>Mon, 27 May 2024 01:48:36 +0800</pubDate><author>zongzel@uchicago.edu (Zongze Li)</author>
      <guid>https://freelulul.github.io/blog/NPUBase_AIsys_intro/</guid>
      <description>NPU 基础(DONE) 近年来，随着人工智能技术的飞速发展，AI 专用处理器如 NPU（Neural Processing Unit）和 TPU（Tensor Processing Unit）也应运而生。这些处理器旨在加速深度学习和机器学习任务，相比传统的 CPU 和 GPU，它们在处理 AI 任务时表现出更高的效率和性能。在接下来的内容中，我们将首先简单介绍引入什么是 AI 芯片，随后具体展开其的部署说明，技术发展路线和应用场景。&#xA;什么是 AI 芯片 AI 芯片是专门为加速人工智能应用中的大量针对矩阵计算任务而设计的处理器或计算模块。与传统的通用芯片如中央处理器（CPU）不同，AI 芯片采用针对特定领域优化的体系结构（Domain-Specific Architecture，DSA），侧重于提升执行 AI 算法所需的专用计算性能。&#xA;如下图所示的就是一个典型的 AI 芯片架构，我们假设所有场景围绕应用，那么其周围的例如解码芯片（如图中黄色部分 RSU）、FPGA 芯片（如图中粉色部分）等都是属于针对特定领域优化的芯片结构。&#xA;DSA 通常被称为针对特殊领域的加速器架构，因为与在通用 CPU 上执行整个应用程序相比，它们可以大幅提升特定应用的性能。DSA 可以通过更贴近应用的实际需求来实现更高的效率和性能。除了 AI 芯片，DSA 的其他例子还包括图形加速单元（GPU）、用于深度学习的神经网络处理器（NPU/TPU）以及软件定义网络（SDN）处理器等。&#xA;AI 芯片作为一种专用加速器，通过在硬件层面优化深度学习算法所需的矩阵乘法、卷积等关键运算，可以显著加速 AI 应用的执行速度，降低功耗。与在通用 CPU 上用软件模拟这些运算相比，AI 芯片能带来数量级的性能提升。因此，AI 芯片已成为人工智能技术实现落地的关键使能器。&#xA;他们的架构区别如下图，CPU 最为均衡，可以处理多种类型的任务，各种组件比例适中；GPU 则减少了控制逻辑的存在但大量增加了 ALU 计算单元，提供给我们以高计算并行度；而 NPU 则是拥有大量 AI Core，这可以让我们高效完成针对性的 AI 计算任务。&#xA;AI 芯片的兴起源于深度学习的快速发展。随着神经网络模型的规模不断增大，其应用快速发展，训练和推理所需的计算量呈指数级增长，传统的通用芯片已无法满足性能和功耗的要求。与此同时，AI 应用对实时性和能效的需求也日益提高，尤其是在自动驾驶、智能安防、边缘计算等场景中。这些因素共同推动了 AI 芯片的发展。&#xA;AI 专用处理器的发展可以追溯到 2016 年，谷歌推出了第一代 TPU，采用了独特的 TPU 核心脉动阵列设计，专门用于加速 TensorFlow 框架下的机器学习任务。此后，谷歌又陆续推出了多个 TPU 系列产品，不断优化其架构和性能。&#xA;华为也紧随其后，推出了自己的 AI 专用处理器——昇腾 NPU。昇腾 NPU 采用了创新的达芬奇架构，集成了大量的 AI 核心，可以高效地处理各种 AI 任务。华为还推出了多款搭载昇腾 NPU 的产品，如华为 Mate 系列手机和 Atlas 服务器等。&#xA;特斯拉作为一家以电动汽车和自动驾驶技术闻名的公司，也推出了自己的 AI 芯片——DOJO。DOJO 采用了独特的架构设计，旨在加速自动驾驶系统的训练和推理任务。&#xA;除了上述几家巨头外，国内外还有许多其他公司也在积极布局 AI 芯片领域，如寒武纪的 MLU 系列、地平线的征程系列等。这些 AI 芯片在架构设计、性能表现、应用场景等方面各有特点，为 AI 技术的发展提供了强有力的硬件支持。</description>
    </item>
    <item>
      <title>GPUBase_AIsys_intro</title>
      <link>https://freelulul.github.io/blog/GPUBase_AIsys_intro/</link>
      <pubDate>Wed, 15 May 2024 01:48:36 +0800</pubDate><author>zongzel@uchicago.edu (Zongze Li)</author>
      <guid>https://freelulul.github.io/blog/GPUBase_AIsys_intro/</guid>
      <description>GPU 基础 (DONE) GPU 是 Graphics Processing Unit（图形处理器）的简称，它是计算机系统中负责处理图形和图像相关任务的核心组件。GPU 的发展历史可以追溯到对计算机图形处理需求的不断增长，以及对图像渲染速度和质量的不断追求。从最初的简单图形处理功能到如今的高性能计算和深度学习加速器，GPU 经历了一系列重要的技术突破和发展转折。&#xA;在接下来的内容中，我们还将探讨 GPU 与 CPU 的区别，了解它们在设计、架构和用途上存在显著差异。此外，我们还将简短介绍一下 AI 发展和 GPU 的联系，并探讨 GPU 在各种领域的应用场景。&#xA;除了图形处理和人工智能，GPU 在科学计算、数据分析、加密货币挖矿等领域也有着广泛的应用。深入了解这些应用场景有助于我们更好地发挥 GPU 的潜力，解决各种复杂计算问题。现在让我们深入了解 GPU 的发展历史、与 CPU 的区别、AI 所需的重要性以及其广泛的应用领域。&#xA;GPU 发展历史 在 GPU 发展史上，第一代 GPU 可追溯至 1999 年之前。这一时期的 GPU 在图形处理领域进行了一定的创新，部分功能开始从 CPU 中分离出来，实现了针对图形处理的硬件加速。其中，最具代表性的是几何处理引擎，即 GEOMETRY ENGINE。该引擎主要用于加速 3D 图像处理，但相较于后来的 GPU，它并不具备软件编程特性。这意味着它的功能相对受限，只能执行预定义的图形处理任务，而无法像现代 GPU 那样灵活地适应不同的软件需求。&#xA;然而，尽管功能有限，第一代 GPU 的出现为图形处理领域的硬件加速打下了重要的基础，奠定了后续 GPU 技术发展的基石。&#xA;第二代 GPU 的发展跨越了 1999 年到 2005 年这段时期，其间取得了显著的进展。1999 年，英伟达发布了 GeForce256 图像处理芯片，这款芯片专为执行复杂的数学和几何计算而设计。与此前的 GPU 相比，GeForce256 将更多的晶体管用于执行单元，而不是像 CPU 那样用于复杂的控制单元和缓存。它成功地将诸如变换与光照（TRANSFORM AND LIGHTING）等功能从 CPU 中分离出来，实现了图形快速变换，标志着 GPU 的真正出现。&#xA;随着时间的推移，GPU 技术迅速发展。从 2000 年到 2005 年，GPU 的运算速度迅速超越了 CPU。在 2001 年，英伟达和 ATI 分别推出了 GeForce3 和 Radeon 8500，这些产品进一步推动了图形硬件的发展。图形硬件的流水线被定义为流处理器，顶点级可编程性开始出现，同时像素级也具有了有限的编程性。&#xA;尽管如此，第二代 GPU 的整体编程性仍然相对有限，与现代 GPU 相比仍有一定差距。然而，这一时期的 GPU 发展为后续的技术进步奠定了基础，为图形处理和计算领域的发展打下了坚实的基础。</description>
    </item>
    <item>
      <title>PowerInfer_AMD_Optimization</title>
      <link>https://freelulul.github.io/blog/PowerInfer_AMD_Optimization/</link>
      <pubDate>Mon, 15 Apr 2024 01:44:02 +0800</pubDate><author>zongzel@uchicago.edu (Zongze Li)</author>
      <guid>https://freelulul.github.io/blog/PowerInfer_AMD_Optimization/</guid>
      <description>Environment: aupxtx@aupxtx:~$ python3 -m torch.utils.collect_env Collecting environment information... PyTorch version: 2.3.0+rocm5.7 Is debug build: False CUDA used to build PyTorch: N/A ROCM used to build PyTorch: 5.7.31921-d1770ee1b OS: Ubuntu 22.04.3 LTS (x86_64) GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 Clang version: 17.0.0 (https://github.com/RadeonOpenCompute/llvm-project roc-6.0.2 24012 af27734ed982b52a9f1be0f035ac91726fc697e4) CMake version: version 3.22.1 Libc version: glibc-2.35 Python version: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] (64-bit runtime) Python platform: Linux-6.2.0-26-generic-x86_64-with-glibc2.35 Is CUDA available: True CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: Radeon RX 7900 XTX (gfx1100) Nvidia driver version: Could not collect cuDNN version: Could not collect HIP runtime version: 5.</description>
    </item>
    <item>
      <title>使用Github Page&#43;Hugo搭建个人主页&amp;自定义修改网页组件心得</title>
      <link>https://freelulul.github.io/blog/%E4%BD%BF%E7%94%A8Github-Page&#43;Hugo%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E4%B8%BB%E9%A1%B5%E8%87%AA%E5%AE%9A%E4%B9%89%E4%BF%AE%E6%94%B9%E7%BD%91%E9%A1%B5%E7%BB%84%E4%BB%B6%E5%BF%83%E5%BE%97/</link>
      <pubDate>Tue, 09 Apr 2024 01:48:36 +0800</pubDate><author>zongzel@uchicago.edu (Zongze Li)</author>
      <guid>https://freelulul.github.io/blog/%E4%BD%BF%E7%94%A8Github-Page&#43;Hugo%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E4%B8%BB%E9%A1%B5%E8%87%AA%E5%AE%9A%E4%B9%89%E4%BF%AE%E6%94%B9%E7%BD%91%E9%A1%B5%E7%BB%84%E4%BB%B6%E5%BF%83%E5%BE%97/</guid>
      <description>0. 引言 博0预备役选手，终于把拖延了很久的学术主页搭建拉上日程，由于不想用完全现成且使用率太高的模板，打算浅浅学一些前端知识配合Github Page和Hugo生成器操作一番（Hugo社区有许多好看且免费的主题！）实际搭建和部署的过程主要参考博主小绵尾巴：如何用 GitHub Pages + Hugo 搭建个人博客分享的教程，其中细节和注意事项非常详细，推荐大家参考。本文旨在原本教程的基础上分享一些Hugo内置组件自定义更改的心得，以实现在原本主题的基础上添加更多个性化的内容的美化目的。&#xA;1. Hugo安装及主题本地部署 环境基于Mac OS，Windows/Linux请参考安装Hugo&#xA;# Install Hugo with Homebrew brew install hugo # Create site folder named &amp;#39;page&amp;#39; hugo new site page cd page git init # Select a Hugo theme, I used &amp;#39;Personal Web:https://themes.gohugo.io/themes/personal-web&amp;#39; as an example git submodule add https://github.com/bjacquemet/personal-web.git themes/personal-web # Copy some files to page directory cp themes/personal-web/exampleSite/config.toml ./hugo.toml cp -r themes/personal-web/exampleSite/content ./ cp -r themes/personal-web/exampleSite/static ./ cp -r themes/personal-web/archetypes ./ # Launch in local hugo server -D 接下来根据bind adress即可在本地打开所部署的网页，一般为http://localhost:1313/&#xA;2. Hugo重要部件解析 2.1 hugo.toml 首先我们先明确Hugo生成网页的本质，它是用 Go 语言写的静态网站生成器，核心逻辑是通过预设的参数和配置文件加上我们自己写的 Markdown 文件，将其统一转化成 HTML 文件，并将生成的网页文件都放在工作目录下的public文件夹中（如果你按照上面的流程成功部署了网页，那你一定可以在public文件夹中找到所有生成的html文件），而我们之后借助Github Page要发布的网站实际就是public文件夹中的内容。&#xA;理解上述后，我们就可以开始自定义修改网页内容了。&#xA;首先，最重要的配置文件就是工作目录下的hugo.toml/config.toml，大多数情况下，我们对网页80%的修改都可以通过调整这之中的参数来完成，且本文件可读性较强，大多数内容都可以直观的看懂并修改，比如示例主题中的 [params.intro], [params.main], [params.sidebar], [params.</description>
    </item>
    <item>
      <title>PowerInfer_AMD_Debug_Log</title>
      <link>https://freelulul.github.io/blog/PowerInfer_AMD_Debug_Log/</link>
      <pubDate>Fri, 05 Apr 2024 01:48:36 +0800</pubDate><author>zongzel@uchicago.edu (Zongze Li)</author>
      <guid>https://freelulul.github.io/blog/PowerInfer_AMD_Debug_Log/</guid>
      <description>Environment: aupxtx@aupxtx:~$ python3 -m torch.utils.collect_env Collecting environment information... PyTorch version: 2.3.0+rocm5.7 Is debug build: False CUDA used to build PyTorch: N/A ROCM used to build PyTorch: 5.7.31921-d1770ee1b OS: Ubuntu 22.04.3 LTS (x86_64) GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 Clang version: 17.0.0 (https://github.com/RadeonOpenCompute/llvm-project roc-6.0.2 24012 af27734ed982b52a9f1be0f035ac91726fc697e4) CMake version: version 3.22.1 Libc version: glibc-2.35 Python version: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] (64-bit runtime) Python platform: Linux-6.2.0-26-generic-x86_64-with-glibc2.35 Is CUDA available: True CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: Radeon RX 7900 XTX (gfx1100) Nvidia driver version: Could not collect cuDNN version: Could not collect HIP runtime version: 5.</description>
    </item>
  </channel>
</rss>
